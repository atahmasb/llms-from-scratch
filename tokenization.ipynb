{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "y_ye68jhJqe6"
      },
      "id": "y_ye68jhJqe6",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    A Byte Pair Encoding (BPE) Tokenizer for training on a corpus and tokenizing text.\n",
        "\n",
        "    Attributes:\n",
        "        corpus (List[str]): The input text corpus for training the tokenizer.\n",
        "        vocab_size (int): The desired size of the vocabulary.\n",
        "        merge_limit (int): Limit on the number of merges performed during training.\n",
        "        word_freq (defaultdict): Frequency of words in the corpus.\n",
        "        splits (dict): Mapping of words to their split representation (character-level).\n",
        "        vocab (List[str]): The vocabulary of the tokenizer.\n",
        "        merge_rules (List[Tuple[str, str]]): List of merge rules learned during training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, corpus: List[str], vocab_size: int, merge_limit: int):\n",
        "        \"\"\"\n",
        "        Initializes the BPE Tokenizer with the given corpus, vocabulary size, and merge limit.\n",
        "\n",
        "        Args:\n",
        "            corpus (List[str]): List of text strings for training.\n",
        "            vocab_size (int): Desired vocabulary size.\n",
        "            merge_limit (int): Maximum number of merges to perform.\n",
        "        \"\"\"\n",
        "        self._corpus = corpus\n",
        "        self._vocab_size = vocab_size\n",
        "        self._merge_limit = merge_limit\n",
        "        self._word_freq = defaultdict(int)\n",
        "        self._splits = dict()\n",
        "        self._vocab = []\n",
        "        self._merge_rules = []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the BPE tokenizer by learning merge rules from the corpus.\n",
        "        \"\"\"\n",
        "        self._prepare_corpus()\n",
        "        while len(self._vocab) < self._vocab_size:\n",
        "            pairs_freq = self._get_pair_freq()\n",
        "            most_freq_pair, freq = self._get_most_freq_pair(pairs_freq)\n",
        "            self._merge(*most_freq_pair)\n",
        "            self._update_vocab(most_freq_pair)\n",
        "            self._update_merge_rules(most_freq_pair)\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenizes the input text using the learned merge rules.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            List[str]: The tokenized representation of the input text.\n",
        "        \"\"\"\n",
        "        tokenized_text = []\n",
        "        words = text.split(\" \")\n",
        "        splits = [[c for c in word] for word in words]\n",
        "\n",
        "        for rule in self._merge_rules:\n",
        "            for idx, split in enumerate(splits):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    a, b = split[i], split[i + 1]\n",
        "                    if a + b == \"\".join(rule):\n",
        "                        split = split[:i] + [a + b] + split[i + 2:]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[idx] = split\n",
        "\n",
        "        return sum(splits, [])\n",
        "\n",
        "    def decode(text: str):\n",
        "        pass\n",
        "\n",
        "    def _prepare_corpus(self):\n",
        "        \"\"\"\n",
        "        Prepares the corpus by calculating word frequencies, creating splits, and building the initial vocabulary.\n",
        "        \"\"\"\n",
        "        self._get_word_freq()\n",
        "        self._create_splits()\n",
        "        self._build_initial_vocab()\n",
        "\n",
        "    def _get_word_freq(self):\n",
        "        \"\"\"\n",
        "        Calculates the frequency of each word in the corpus.\n",
        "        \"\"\"\n",
        "        for text in self._corpus:\n",
        "            words = text.split(\" \")\n",
        "            for word in words:\n",
        "                self._word_freq[word] += 1\n",
        "\n",
        "    def _create_splits(self):\n",
        "        \"\"\"\n",
        "        Splits each word in the corpus into characters.\n",
        "        \"\"\"\n",
        "        self._splits = {word: [c for c in word] for word in self._word_freq}\n",
        "\n",
        "    def _build_initial_vocab(self):\n",
        "        \"\"\"\n",
        "        Builds the initial vocabulary consisting of unique characters in the corpus.\n",
        "        \"\"\"\n",
        "        for word, splits in self._splits.items():\n",
        "            for c in splits:\n",
        "                if c not in self._vocab:\n",
        "                    self._vocab.append(c)\n",
        "\n",
        "    def _get_pair_freq(self) -> defaultdict:\n",
        "        \"\"\"\n",
        "        Calculates the frequency of adjacent character pairs in the corpus.\n",
        "\n",
        "        Returns:\n",
        "            defaultdict: Frequency of character pairs.\n",
        "        \"\"\"\n",
        "        pairs_freq = defaultdict(int)\n",
        "        for word, freq in self._word_freq.items():\n",
        "            splits = self._splits[word]\n",
        "            for i in range(len(splits) - 1):\n",
        "                pair = (splits[i], splits[i + 1])\n",
        "                pairs_freq[pair] += freq\n",
        "        return pairs_freq\n",
        "\n",
        "    def _get_most_freq_pair(self, pairs_freq: defaultdict) -> Tuple[Tuple[str, str], int]:\n",
        "        \"\"\"\n",
        "        Finds the most frequent character pair in the corpus.\n",
        "\n",
        "        Args:\n",
        "            pairs_freq (defaultdict): Frequency of character pairs.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Tuple[str, str], int]: The most frequent pair and its frequency.\n",
        "        \"\"\"\n",
        "        most_freq = None\n",
        "        max_freq = None\n",
        "        for pair, freq in pairs_freq.items():\n",
        "            if most_freq is None or freq > max_freq:\n",
        "                most_freq = pair\n",
        "                max_freq = freq\n",
        "        return most_freq, max_freq\n",
        "\n",
        "    def _merge(self, a: str, b: str):\n",
        "        \"\"\"\n",
        "        Merges a pair of characters in all words in the corpus.\n",
        "\n",
        "        Args:\n",
        "            a (str): The first character of the pair.\n",
        "            b (str): The second character of the pair.\n",
        "        \"\"\"\n",
        "        for word in self._word_freq.keys():\n",
        "            split = self._splits[word]\n",
        "\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "\n",
        "            i = 0\n",
        "            while i < len(split) - 1:\n",
        "                first_element, second_element = split[i], split[i + 1]\n",
        "                if first_element == a and second_element == b:\n",
        "                    split = split[:i] + [a + b] + split[i + 2:]\n",
        "                else:\n",
        "                    i += 1\n",
        "            self._splits[word] = split\n",
        "\n",
        "    def _update_vocab(self, pair: Tuple[str, str]):\n",
        "        \"\"\"\n",
        "        Adds a merged pair to the vocabulary.\n",
        "\n",
        "        Args:\n",
        "            pair (Tuple[str, str]): The merged character pair.\n",
        "        \"\"\"\n",
        "        self._vocab.append(\"\".join(pair))\n",
        "\n",
        "    def _update_merge_rules(self, merge_rule: Tuple[str, str]):\n",
        "        \"\"\"\n",
        "        Updates the list of merge rules with the new rule.\n",
        "\n",
        "        Args:\n",
        "            merge_rule (Tuple[str, str]): The new merge rule.\n",
        "        \"\"\"\n",
        "        self._merge_rules.append(merge_rule)\n"
      ],
      "metadata": {
        "id": "iTLLyAAyJsVQ"
      },
      "id": "iTLLyAAyJsVQ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"low lower lowest\",\n",
        "\"new newer newest\",\n",
        "\"widest wide\",\n",
        "\"brightest bright\",\n",
        "\n",
        "]\n",
        "\n",
        "vocab_size = 20\n",
        "merge_limit = 10"
      ],
      "metadata": {
        "id": "X9jvCeCx7ljJ"
      },
      "id": "X9jvCeCx7ljJ",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BPETokenizer(corpus=corpus, vocab_size=vocab_size, merge_limit=merge_limit)"
      ],
      "metadata": {
        "id": "OjG_-Isi7obd"
      },
      "id": "OjG_-Isi7obd",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train()"
      ],
      "metadata": {
        "id": "FkwdIxbc7qvD"
      },
      "id": "FkwdIxbc7qvD",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"lowing low bri\")"
      ],
      "metadata": {
        "id": "CRPqLgF07tM2",
        "outputId": "27c1e748-7e78-411f-94fa-379a5340f67b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CRPqLgF07tM2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lo', 'wi', 'n', 'g', 'lo', 'w', 'b', 'r', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkzPcmsp7vEy"
      },
      "id": "LkzPcmsp7vEy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}